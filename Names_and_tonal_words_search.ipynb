{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pymorphy2\n",
    "import razdel\n",
    "# import tokenization\n",
    "from stop_words import get_stop_words\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import six\n",
    "import razdel\n",
    "\n",
    "\n",
    "ACCENT = six.unichr(769)\n",
    "WORD_TOKENIZATION_RULES = re.compile(r\"\"\"\n",
    "[\\w\"\"\" + ACCENT + \"\"\"]+://(?:[a-zA-Z]|[0-9]|[$-_@.&+])+\n",
    "|[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+\n",
    "|[0-9]+-[а-яА-ЯіїІЇєЄґҐ'’`\"\"\" + ACCENT + \"\"\"]+\n",
    "|[+-]?[0-9](?:[0-9,.-]*[0-9])?\n",
    "|[\\w\"\"\" + ACCENT + \"\"\"](?:[\\w'’`-\"\"\" + ACCENT + \"\"\"]?[\\w\"\"\" + ACCENT + \"\"\"]+)*\n",
    "|[\\w\"\"\" + ACCENT + \"\"\"].(?:\\[\\w\"\"\" + ACCENT + \"\"\"].)+[\\w\"\"\" + ACCENT + \"\"\"]?\n",
    "|[\"#$%&*+,/:;<=>@^`~…\\\\(\\\\)⟨⟩{}\\[\\|\\]‒–—―«»“”‘’'№]\n",
    "|[.!?]+\n",
    "|-+\n",
    "\"\"\", re.X | re.U)\n",
    "\n",
    "\n",
    "ABBRS = \"\"\"\n",
    "ім.\n",
    "в.\n",
    "о.\n",
    "т.\n",
    "п.\n",
    "д.\n",
    "под.\n",
    "ін.\n",
    "вул.\n",
    "просп.\n",
    "бул.\n",
    "пров.\n",
    "пл.\n",
    "г.\n",
    "р.\n",
    "див.\n",
    "п.\n",
    "с.\n",
    "м.\n",
    "н.\n",
    "е.\n",
    "адмін.\n",
    "к.\n",
    "геогр.\n",
    "обл.\n",
    "смт.\n",
    "авт.\n",
    "адм.\n",
    "акад.\n",
    "англ.\n",
    "арк.\n",
    "арт.\n",
    "археол.\n",
    "арх.\n",
    "архіт.\n",
    "асист.\n",
    "асоц.\n",
    "б.\n",
    "буд.\n",
    "бух.\n",
    "бюдж.\n",
    "вет.\n",
    "вид.\n",
    "викл.\n",
    "відкр.\n",
    "дип.\n",
    "діагр.\n",
    "екол.\n",
    "екон.\n",
    "євр.\n",
    "журн.\n",
    "зобр.\n",
    "іл.\n",
    "інв.\n",
    "інд.\n",
    "інж.\n",
    "іст.\n",
    "каф.\n",
    "кл.\n",
    "коеф.\n",
    "лаб.\n",
    "лінгв.\n",
    "літ.\n",
    "мат.\n",
    "мед.\n",
    "мех.\n",
    "міс.\n",
    "муз.\n",
    "нар.\n",
    "нац.\n",
    "орг.\n",
    "офіц.\n",
    "пед.\n",
    "пр.\n",
    "проф.\n",
    "публ.\n",
    "рис.\n",
    "мал.\n",
    "pp.\n",
    "рос.\n",
    "св.\n",
    "сл.\n",
    "ст.\n",
    "студ.\n",
    "табл.\n",
    "тис.\n",
    "укр.\n",
    "упр.\n",
    "фіз.\n",
    "фін.\n",
    "ц.\n",
    "\"\"\".strip().split()\n",
    "\n",
    "\n",
    "def tokenize_sents(string):\n",
    "    string = six.text_type(string)\n",
    "    spans = []\n",
    "    for match in re.finditer('[^\\s]+', string):\n",
    "        spans.append(match)\n",
    "    spans_count = len(spans)\n",
    "\n",
    "    rez = []\n",
    "    off = 0\n",
    "\n",
    "    for i in range(spans_count):\n",
    "        tok = string[spans[i].start():spans[i].end()]\n",
    "        if i == spans_count - 1:\n",
    "            rez.append(string[off:spans[i].end()])\n",
    "        elif tok[-1] in ['.', '!', '?', '…', '»', \"'\", \"\\\"\"]:\n",
    "            # tok1 = tok[re.search('[.!?…»]', tok).start() - 1]\n",
    "            next_tok = string[spans[i + 1].start():spans[i + 1].end()]\n",
    "            if (next_tok[0].isupper() or next_tok[0] in [\"'\", \"\\\"\", \"«\"]) \\\n",
    "                    and not ((len(tok) == 2 and tok[0].isupper()) \\\n",
    "                             or tok[0] == '('\n",
    "                             or tok in ABBRS):\n",
    "                rez.append(string[off:spans[i].end()])\n",
    "                off = spans[i + 1].start()\n",
    "\n",
    "    return rez\n",
    "\n",
    "\n",
    "def text_to_sent(text, lang):\n",
    "    rez = []\n",
    "    if lang == 'uk':\n",
    "        for part in text.split('\\n'):\n",
    "            rez += tokenize_sents(part)\n",
    "    elif lang=='ru':\n",
    "        for part in text.split('\\n'):\n",
    "            rez += [s.text for s in razdel.sentenize(part)]\n",
    "    return rez\n",
    "\n",
    "\n",
    "def sent_to_words(text, lang):\n",
    "    if lang == 'uk':\n",
    "        return re.findall(WORD_TOKENIZATION_RULES, text)\n",
    "    elif lang == 'ru':\n",
    "        return [tkn.text for tkn in razdel.tokenize(text)]\n",
    "    return None\n",
    "\n",
    "\n",
    "def tokenize(text, lang):\n",
    "    res = []\n",
    "    for sent in text_to_sent(text, lang):\n",
    "        tokens = []\n",
    "        for word in sent_to_words(sent, lang):\n",
    "            tokens.append(word)\n",
    "        res.append(tokens)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Names search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_filepath = '../data/april.csv'\n",
    "ukr_tone_filepath = 'dicts/EmotionLookupTable_ukr.txt'\n",
    "ru_tone_filepath = 'dicts/EmotionLookupTable_ru.txt'\n",
    "\n",
    "ukr_politician_dict_path = 'dicts/politicians_ukr.csv'\n",
    "ru_politician_dict_path = 'dicts/politicians_ru.csv'\n",
    "\n",
    "ukr_names_filepath = 'dicts/names_ukr.txt'\n",
    "ru_names_filepath = 'dicts/names_ru.txt'\n",
    "\n",
    "tone_changers_filepath = 'dicts/tone_changers.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ukr = get_stop_words('uk')\n",
    "stop_words_ru = get_stop_words('ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(news_filepath)\n",
    "news['all_text'] = news.title.str.cat(news.text, sep='\\n', na_rep = '')\n",
    "news['all_text'] = news.all_text.str.strip()\n",
    "news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians_dict_ukr = pd.read_csv(ukr_politician_dict_path, sep=';')\n",
    "politicians_dict_ru = pd.read_csv(ru_politician_dict_path, sep=';')\n",
    "tone_dict_ukr = pd.read_csv(ukr_tone_filepath, sep='\\t', header=None, names=['word', 'tone'])\n",
    "tone_dict_ru = pd.read_csv(ru_tone_filepath, sep='\\t', header=None, names=['word', 'tone'])\n",
    "morph_ukr = pymorphy2.MorphAnalyzer(lang='uk')\n",
    "morph_ru = pymorphy2.MorphAnalyzer(lang='ru')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tone_changers_filepath) as f:\n",
    "    tone_changers=f.read()\n",
    "\n",
    "tone_changers = json.loads(tone_changers)\n",
    "negations_ukr = tone_changers['ukr']['negation']\n",
    "intensifiers_ukr = tone_changers['ukr']['intensifier']\n",
    "negations_ru = tone_changers['ru']['negation']\n",
    "intensifiers_ru = tone_changers['ru']['intensifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text into sentences\n",
    "def get_text_in_sents(news):\n",
    "    news = news[news.text.notna()]\n",
    "    news = news[(news.language=='uk')|(news.language=='ru')]\n",
    "    news['text'] = news.apply(lambda row: text_to_sent(row.text, row.language), axis=1)\n",
    "    return news  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences to words\n",
    "def get_tokenized_sentences(news):\n",
    "    news['tokenized'] = news.apply(lambda row: tokenize(row.all_text, row.language), axis=1)\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 2.71 s, total: 1min 18s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news['sentenized'] = news.apply(lambda row: text_to_sent(row.all_text, row.language), axis=1)\n",
    "# news = get_tokenized_sentences(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming different name variations to search in text:\n",
    "# name + surname\n",
    "# surname + name + patronim\n",
    "# name + patronim + surname\n",
    "# surname + name\n",
    "# surname \n",
    "# and list of wrong names preceding surname    \n",
    "def form_name_pattern(politician, ending, all_names):\n",
    "\n",
    "    names_forms = []\n",
    "    names_forms.append(rf\"({politician['name']} {politician['surname']})\")\n",
    "\n",
    "\n",
    "    if pd.notnull(politician.patronim):\n",
    "        names_forms.append(rf\"({politician['surname']} {politician['name']}(?: {politician['patronim']})?)\")\n",
    "        names_forms.append(rf\"((?:{politician['surname']} )?{politician['name']} {politician['patronim']}(?: {politician['surname']})?)\")\n",
    "    else:\n",
    "        names_forms.append(rf\"({politician['surname']} {politician['name']})\")\n",
    "    names_forms.append(rf\"({politician['surname']})\")\n",
    "    wrong_names = [n for n in all_names if n != politician['shortname']] \n",
    "    wrong_names = '|'.join(wrong_names)\n",
    "    wrong_names_forms = rf\"((?:{wrong_names}){ending} {politician['surname']})\"\n",
    "\n",
    "    return '|'.join(names_forms), wrong_names_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add endings to stemmed names to cover possible cases\n",
    "ending_ukr = r\"[аийуяіеоюєїьмфцвскпнртгшлдщзбжх]{,3}\\b\"\n",
    "ending_ru = r\"[аийуыяеоюёьэмфцвскпнртгшлдщзбжх]{,3}\\b\"\n",
    "\n",
    "\n",
    "def add_endings(name_dict, ending, all_names):\n",
    "    name_forms_dict = name_dict[['surname','name','patronim']].applymap(lambda x: x+ending if pd.notnull(x) else None)\n",
    "    name_forms_dict['fullname'] = name_dict.fullname\n",
    "    name_forms_dict['shortname'] = name_dict['name']\n",
    "    name_forms_dict['name_forms'], name_forms_dict['wrong_names'] = zip(*name_forms_dict.apply(form_name_pattern, \n",
    "                                                                                               ending=ending, \n",
    "                                                                                               all_names=all_names, axis=1))\n",
    "    return name_forms_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ukr_names_filepath) as f:\n",
    "    all_names_ukr = [n.strip() for n in f.readlines()]\n",
    "\n",
    "with open(ru_names_filepath) as f:\n",
    "    all_names_ru = [n.strip() for n in f.readlines()]\n",
    "\n",
    "    \n",
    "name_forms_ukr = add_endings(politicians_dict_ukr, ending_ukr, all_names_ukr)\n",
    "name_forms_ru = add_endings(politicians_dict_ru, ending_ru, all_names_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for names in sentence and return result as a string\n",
    "def name_search_in_sentence(sentence, name_forms_dict):\n",
    "    ments = name_forms_dict.name_forms.apply(lambda x: len(re.findall(x, sentence)))\n",
    "    part = name_forms_dict[ments > 0]\n",
    "    wrongs = part.wrong_names.apply(lambda x: len(re.findall(x, sentence)))\n",
    "    return ';'.join(list(part.fullname.str.cat((ments - wrongs).astype(str), '+')))\n",
    "\n",
    "\n",
    "# concatenate sentence results into one string for whole text\n",
    "def name_search_in_text(row, name_forms_ukr = name_forms_ukr, name_froms_ru = name_forms_ru):\n",
    "    if row.language == 'uk':\n",
    "        mentions = [name_search_in_sentence(sent, name_forms_ukr) for sent in row.sentenized]\n",
    "    else:    \n",
    "        mentions = [name_search_in_sentence(sent, name_froms_ru) for sent in row.sentenized]\n",
    "    if any(mentions):\n",
    "        return '§'.join(mentions)\n",
    "    return None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['mentions'] = news.apply(name_search_in_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mentions just in case\n",
    "news[['link', 'id','mentions']].to_csv('april_mentions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[['title', 'text', 'subtitle', 'link', 'domain', 'datetime', 'views',\n",
    "       'created_at', 'category', 'language', 'id', 'domain_alias',\n",
    "       'mycategory', 'mentions']].to_csv('../data/april.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tonal words search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ця частина вже зайва, бо емоційне забарвлення визначаємо окремою моделлю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing token tonality if it's negated or intensified\n",
    "def change_tone(tokens, ind, tone, morph, negations, intensifiers):\n",
    "    if ind > 0:\n",
    "        if tokens[ind-1] in negations or\\\n",
    "        (ind > 1 and tokens[ind-2] == 'не' and morph.parse(tokens[ind-1])[0].normal_form in negations[-1]):\n",
    "            return tone*(-1)\n",
    "       \n",
    "        if morph.parse(tokens[ind-1])[0].normal_form in intensifiers:\n",
    "            if tone > 0:\n",
    "                return tone+1\n",
    "            else:\n",
    "                return tone-1\n",
    "    return tone\n",
    "\n",
    "\n",
    "# search for non-neutral words in a sentence tokenized to list of words\n",
    "def count_tone(tokens, morph, tone_dict, stop_words, negations, intensifiers):\n",
    "    tone_list = [] \n",
    "    for i in range(len(tokens)):\n",
    "        tok = tokens[i].lower()\n",
    "        if (len(tok) > 2 and tok not in stop_words) or tok=='яд':\n",
    "            tok = morph.parse(tok)[0].normal_form\n",
    "            tone = tone_dict[tone_dict['word'] == tok].tone\n",
    "            if not tone.empty:\n",
    "                try:\n",
    "                    tone = int(tone)\n",
    "                    tone = change_tone(tokens, i, tone, morph, negations, intensifiers)\n",
    "                    tone_list.append(tok + ':' + str(tone))\n",
    "                except TypeError:\n",
    "                    print(tone)\n",
    "                    pass\n",
    "                del tone\n",
    "        else:\n",
    "            del tok\n",
    "    return tone_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['tokenized'] = news.apply(lambda row: [sent_to_words(sent, row.language) for sent in row.sentenized], \n",
    "                               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all non-netrual words in text(with their tonality) as a string\n",
    "# join tonal words in sentence by ';' and then join sentence results by '#'\n",
    "def tone_by_text(row, ukr_tone=tone_dict_ukr, ru_tone=tone_dict_ru, ukr_stop=stop_words_ukr, ru_stop=stop_words_ru):\n",
    "    if row.language == 'uk':\n",
    "        return '#'.join([';'.join(count_tone(sent, morph_ukr, ukr_tone, ukr_stop, negations_ukr, intensifiers_ukr))\n",
    "                         for sent in row.tokenized])\n",
    "    else:\n",
    "        return '#'.join([';'.join(count_tone(sent, morph_ru, ru_tone, ru_stop, negations_ru, intensifiers_ru)) \n",
    "                         for sent in row.tokenized])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply your function to the part of dataframe and add result to file\n",
    "def process_parts(news_df, func, out_file, start, stop, step):\n",
    "    res_list = []\n",
    "    for i in range(start, stop, step):\n",
    "        res_part = news_df.iloc[i:i+step].apply(func, axis=1)\n",
    "        res_list.append(res_part)\n",
    "        res_part.to_csv(out_file, mode=\"a\", header=False)\n",
    "        del res_part\n",
    "        \n",
    "    return pd.concat(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 1.14 s, total: 1min 38s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news['tone_by_sents'] = news.apply(tone_by_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'text', 'subtitle', 'link', 'domain', 'datetime', 'views',\n",
       "       'created_at', 'category', 'language', 'id', 'domain_alias',\n",
       "       'mycategory', 'all_text', 'sentenized', 'mentions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[['title', 'text', 'subtitle', 'link', 'domain', 'datetime', 'views',\n",
    "       'created_at', 'category', 'language', 'id', 'domain_alias',\n",
    "       'mycategory', 'mentions']].to_csv('../data/april.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

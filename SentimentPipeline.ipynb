{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal dependencies with __[Stanza](https://stanfordnlp.github.io/stanza/#getting-started)__ and __[spacy_conll](https://spacy.io/universe/project/spacy-conll)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Побудова дерев залежностей текстів новин і збереження їх у форматі conll для подальшої обробки моделлю, що визначає тональність"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import razdel\n",
    "import stanza\n",
    "import six\n",
    "from spacy_conll import init_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oksana/Dev/TextClassification/venv/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "/Users/oksana/Dev/TextClassification/venv/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  del sys.path[0]\n",
      "/Users/oksana/Dev/TextClassification/venv/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "news_filepath = '/Users/oksana/Dev/data/may.csv'\n",
    "news = pd.read_csv(news_filepath)\n",
    "\n",
    "news['title'] = news.title.str.replace(r'\\n', ' ')\n",
    "news['all_text'] = news.title.str.cat(news.subtitle, sep='\\n', na_rep = '')\n",
    "news['all_text'] = news.all_text.str.cat(news.text, sep='\\n', na_rep = '')\n",
    "news['all_text'] = news.all_text.str.strip()\n",
    "\n",
    "news = news.drop(columns=['title', 'subtitle', 'text'])\n",
    "\n",
    "news['all_text'] = news['all_text'].str.replace('не очень', 'неочень')\n",
    "news['all_text'] = news['all_text'].str.replace('не дуже', 'недуже')\n",
    "\n",
    "news.all_text = news.all_text.str.replace(r'^Редактор Цензор\\.НЕТ\\n', '', flags=re.M)\n",
    "news.all_text = news.all_text.str.replace(r'Цензор\\.НЕТ', 'Цензор')\n",
    "news.all_text = news.all_text.str.replace(r'Еспресо\\.Захід', 'Еспресо Захід')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 13.4MB/s]                    \n",
      "2021-10-08 13:15:37 INFO: Downloading default packages for language: uk (Ukrainian)...\n",
      "2021-10-08 13:15:38 INFO: File exists: /Users/oksana/stanza_resources/uk/default.zip.\n",
      "2021-10-08 13:15:43 INFO: Finished downloading models and saved to /Users/oksana/stanza_resources.\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 11.3MB/s]                    \n",
      "2021-10-08 13:15:43 INFO: Downloading these customized packages for language: ru (Russian)...\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| pretrain  | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-10-08 13:15:43 INFO: File exists: /Users/oksana/stanza_resources/ru/tokenize/gsd.pt.\n",
      "2021-10-08 13:15:43 INFO: File exists: /Users/oksana/stanza_resources/ru/pos/gsd.pt.\n",
      "2021-10-08 13:15:43 INFO: File exists: /Users/oksana/stanza_resources/ru/lemma/gsd.pt.\n",
      "2021-10-08 13:15:44 INFO: File exists: /Users/oksana/stanza_resources/ru/depparse/gsd.pt.\n",
      "2021-10-08 13:15:44 INFO: File exists: /Users/oksana/stanza_resources/ru/pretrain/gsd.pt.\n",
      "2021-10-08 13:15:44 INFO: Finished downloading models and saved to /Users/oksana/stanza_resources.\n",
      "2021-10-08 13:15:44 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| lemma     | iu      |\n",
      "=======================\n",
      "\n",
      "2021-10-08 13:15:44 INFO: Use device: cpu\n",
      "2021-10-08 13:15:44 INFO: Loading: tokenize\n",
      "2021-10-08 13:15:44 INFO: Loading: lemma\n",
      "2021-10-08 13:15:44 INFO: Done loading processors!\n",
      "2021-10-08 13:15:44 INFO: Loading these models for language: uk (Ukrainian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | iu      |\n",
      "| pos       | iu      |\n",
      "| lemma     | iu      |\n",
      "| depparse  | iu      |\n",
      "=======================\n",
      "\n",
      "2021-10-08 13:15:44 INFO: Use device: cpu\n",
      "2021-10-08 13:15:44 INFO: Loading: tokenize\n",
      "2021-10-08 13:15:44 INFO: Loading: pos\n",
      "2021-10-08 13:15:46 INFO: Loading: lemma\n",
      "2021-10-08 13:15:46 INFO: Loading: depparse\n",
      "2021-10-08 13:15:47 INFO: Done loading processors!\n",
      "2021-10-08 13:15:48 INFO: Loading these models for language: ru (Russian):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2021-10-08 13:15:48 INFO: Use device: cpu\n",
      "2021-10-08 13:15:48 INFO: Loading: tokenize\n",
      "2021-10-08 13:15:48 INFO: Loading: pos\n",
      "2021-10-08 13:15:49 INFO: Loading: lemma\n",
      "2021-10-08 13:15:49 INFO: Loading: depparse\n",
      "2021-10-08 13:15:51 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('uk')\n",
    "stanza.download('ru', package='gsd', processors='tokenize,pos,lemma,depparse')\n",
    "\n",
    "nlp = stanza.Pipeline('uk', processors='tokenize,lemma')\n",
    "\n",
    "nlp_uk = init_parser(\n",
    "        \"stanza\",\n",
    "        \"uk\", \n",
    "        include_headers=False,\n",
    "        parser_opts = {'processors': 'tokenize,pos,lemma,depparse'}\n",
    "    )\n",
    "\n",
    "nlp_ru = init_parser(\n",
    "        \"stanza\",\n",
    "        \"ru\", \n",
    "        include_headers=False,\n",
    "        parser_opts = {'processors': 'tokenize,pos,lemma,depparse'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tТакий\tтакий\tDET\tPd--m-sna\t_\t2\tdet\t_\t_\n",
      "2\tпрогноз\tпрогноз\tNOUN\tNcmsnn\t_\t3\tnsubj\t_\t_\n",
      "3\tозвучив\tозвучити\tVERB\tVmeis-sm\t_\t0\troot\t_\t_\n",
      "4\tв\tв\tADP\tSpsl\t_\t5\tcase\t_\t_\n",
      "5\tефірі\tефір\tNOUN\tNcmsln\t_\t3\tobl\t_\t_\n",
      "6\t\"Україна\t\"україна\tNOUN\tNcfsny\t_\t3\tnsubj\t_\t_\n",
      "7\t24\"\t24\"\tPUNCT\tU\t_\t8\tdiscourse\t_\t_\n",
      "8\tпрезидент\tпрезидент\tNOUN\tNcmsny\t_\t6\tappos\t_\t_\n",
      "9\tВсеукраїнської\tвсеукраїнський\tADJ\tAo-fsgf\t_\t10\tamod\t_\t_\n",
      "10\tасоціації\tасоціація\tNOUN\tNcfsgn\t_\t8\tnmod\t_\t_\n",
      "11\tкомпаній\tкомпанія\tNOUN\tNcfpgn\t_\t10\tnmod\t_\t_\n",
      "12\tз\tз\tADP\tSpsg\t_\t14\tcase\t_\t_\n",
      "13\tміжнародного\tміжнародний\tADJ\tAo-nsgf\t_\t14\tamod\t_\t_\n",
      "14\tпрацевлаштування\tпрацевлаштування\tNOUN\tNcnsgn\t_\t8\tnmod\t_\t_\n",
      "15\tВасиль\tВасиль\tPROPN\tNpmsny\t_\t6\tflat:title\t_\t_\n",
      "16\tВоскобойник\tВоскобойник\tPROPN\tNpmsny\t_\t15\tflat:name\t_\tSpaceAfter=No\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1=\"Такий прогноз озвучив в ефірі \\\"Україна 24\\\" президент Всеукраїнської асоціації компаній з міжнародного працевлаштування Василь Воскобойник\"\n",
    "doc = nlp_uk(s1)\n",
    "conll = doc._.conll_str\n",
    "print(conll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27434, 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_uk = news[news.mentions.notna()&(news.language=='uk')]\n",
    "news_ru = news[news.mentions.notna()&(news.language=='ru')]\n",
    "news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кожну новину, у якій є згадки політиків, обробляємо і зберігаємо у conll файл з ім'ям, що відповідає індексу новини"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_conllu(doc, news_id, out_dir='/Users/oksana/Dev/Politicians_in_media/may_conllu/conllu_uk/'):\n",
    "    out_file = out_dir + \"{}.conll\".format(news_id)\n",
    "    for sent_idx, sent in enumerate(doc.sents, 1):\n",
    "        header = ['### ', sent_idx, news_id]\n",
    "        pd.DataFrame([header]).to_csv(out_file, sep='\\t', index=False, header=None, mode='a')\n",
    "        sent._.conll_pd.to_csv(out_file, index=False, sep=\"\\t\", encoding='utf-8', mode='a', header=None)\n",
    "        with open(out_file, 'a') as f:\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_part = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обробка займає багато часу, тому запускаємо по черзі на частинах масиву (паралельно іншу частину можна обробляти в Colab)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_news(news, nlp=nlp_uk, start=0, finish=1000, step=100, out_dir='/Users/oksana/Dev/Politicians_in_media/may_conllu/conllu_uk/'):\n",
    "    for k in range(start, finish, step):\n",
    "        news_part = news.iloc[k:k + step].copy()\n",
    "        news_part['docs'] = news_part.tokenized.apply(nlp)\n",
    "        news_part.apply(lambda row: save_to_conllu(row.docs, row['id'], out_dir), axis=1)\n",
    "        print(k+step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "process_news(news_uk, nlp_uk, start=12453, finish=13000, step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "process_news(news_ru, nlp_ru, start=0, finish=len(news_ru), step=200, out_dir='/Users/oksana/Dev/Politicians_in_media/may_conllu/conllu_ru/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis with __[UUUSA](https://github.com/aghie/uuusa)__   ( __[article](https://arxiv.org/abs/1606.05545)__,  __[manual](http://grupolys.org/software/UUUSA/uuusa-user-manual.pdf)__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Далі отримані conll файл подаємо на вхід моделі UUUSA (у файлі samulan-0.1.0.jar)**\n",
    "Вказуємо для неї параметри:\n",
    " - папку із тональними словниками для української/російської мови\n",
    " - файл з конфігураціями\n",
    " - сам conll файл\n",
    " - властивості моделі\n",
    " - verbose\n",
    " - вихідний файл, куди збережеться результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java -Dfile.encoding=UTF-8 -jar -Xmx2g samulan-0.1.0.jar \\ \n",
    "-s UkSentiData \\\n",
    "-r configuration_uk.xml \\\n",
    "-c parsed.conll \\\n",
    "-p samulan.properties \\\n",
    "-v true\n",
    "-o output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in *.conll; do java -Dfile.encoding=UTF-8 -jar -Xmx2g ../samulan-0.1.1.jar \\\n",
    "-s ../UkSentiData \\\n",
    "-r ../configuration_uk.xml \\\n",
    "-c $file \\\n",
    "-p ../samulan.properties\\\n",
    "-sc so \\\n",
    "-o ../output/$file; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вихідний файл має такий вигляд (перший стовпчик - тональність речення)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.0\t\tКаменських назвала свій головний привід для радості , і це не Потап : \"\"\"\" Згодні ? \"\"\"\"\t [The analysis took: 0.060065182 seg.] [Accumulated time is: 0.060065182]\n",
    "    3.0\t\tПопулярна українська співачка Настя Каменських , яка минулого року вийшла заміж за свого колегу Олексія Потапенка , часто радує публіку яскравими постами в своєму Instagram .\t [The analysis took: 0.001861094 seg.] [Accumulated time is: 0.061926276]\n",
    "    1.0\t\tНа карантині у зірки з'явилося більше часу для спілкування з фанатами , чим вона активно користується .\t [The analysis took: 0.001455214 seg.] [Accumulated time is: 0.06338149]\n",
    "    0.0\t\tЧитайте Знай в Google News !\t [The analysis took: 7.0612E-4 seg.] [Accumulated time is: 0.06408761\n",
    "    2.0\t\tНещодавно дружина Потапа розмістила свіже фото , на якому позує біля пальми в легкому жіночному образі – жовтих шортах , білій сорочці та верху від купальника теж білого кольору .\t [The analysis took: 0.002728221 seg.] [Accumulated time is: 0.066815831]\n",
    "    1.0\t\tУ підписі Каменських зазначила , що п'ятниця вже є приводом для радості .\t [The analysis took: 0.002728221 seg.] [Accumulated time is: 0.066815831]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Зчитуємо потім ці файли з тональністю, зберігаємо результат - сумарну тональність новини (sentiment) і список (точніше рядок чисел, що розділені через ;) тональностей окремих її речень (sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_files = os.listdir(sentiment_dir)\n",
    "res = []\n",
    "for file in sentiment_files:\n",
    "    s = pd.read_csv(file, sep='\\t', usecols=[0], header=None)[0]\n",
    "    res.append((file.strip('.conll'), s.sum(), s.astype(str).str.cat(sep=';')))\n",
    "df = pd.DataFrame(res, columns=['id', 'sentiment', 'sent_list'])\n",
    "df.id = df.id.astype('int64')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
